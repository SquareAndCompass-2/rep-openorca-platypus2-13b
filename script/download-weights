#!/usr/bin/env python
import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
CACHE_DIR = 'cache'

tokenizer = AutoTokenizer.from_pretrained(
    "Open-Orca/OpenOrca-Platypus2-13B",
    use_cache=CACHE_DIR,
)
model = AutoModelForCausalLM.from_pretrained(
    "Open-Orca/OpenOrca-Platypus2-13B",
    torch_dtype=torch.bfloat16,
    low_cpu_mem_usage=False,
    use_cache=CACHE_DIR,
)
